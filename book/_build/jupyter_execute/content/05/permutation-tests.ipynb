{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Tests\n",
    "---\n",
    "\n",
    "## Permutation Tests Overview\n",
    "\n",
    "See the **permutation test chapter** in [Inferential Thinking](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring similarity of distributions\n",
    "\n",
    "Permutation tests assess the likelihood that two observed distributions originate from the same process. This assessment requires measuring the similarity of two distributions. Below are test-statistics used for this purpose.\n",
    "\n",
    "In each of the following sections, assume `table` is a dataframe containing two distributions in tidy-form:\n",
    "\n",
    "|Value|Group|\n",
    "|---|---|\n",
    "|2.23|A|\n",
    "|2.13|A|\n",
    "|5.23|B|\n",
    "|...|..|\n",
    "|2.34|B|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference of means\n",
    "\n",
    "If two distributions have different means, then they must be different distributions. This is a coarse measurement of similarity, as different distribution might have identical means.\n",
    "\n",
    "To calculate the difference of means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_of_means = (\n",
    "    table\n",
    "    .pivot_table(index='Group', aggfunc='mean')\n",
    "    .diff()\n",
    "    .dropna(how='all')\n",
    "    .squeeze()\n",
    ")\n",
    "\n",
    "diff_of_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation Distance\n",
    "\n",
    "If the distributions being compared are categorical, then taking a mean doesn't make sense. In this case, one can use the *total variation distance* (or TVD). This test statistic measures the difference in proportions represented by each category across the two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = (\n",
    "    table\n",
    "    .pivot_table(index='Value', columns='Group', aggfunc='size', fill_value=0)\n",
    "    .apply(lambda x: x / x.sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvd = (\n",
    "    distr\n",
    "    .diff(axis=1)\n",
    "    .dropna(axis=1, how='all')\n",
    "    .squeeze()\n",
    "    .abs()\n",
    "    .sum()\n",
    ") / 2\n",
    "\n",
    "tvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark:* See [inferential thinking](https://www.inferentialthinking.com/chapters/11/2/Multiple_Categories.html#A-New-Statistic:-The-Distance-between-Two-Distributions) for more on this test statistic.\n",
    "\n",
    "*Remark:* The *absolute* difference is necessary; the computation would always sum to zero without it. This is because a positive difference between categories must always be balanced by a negative difference in another category. This observation is a consequence of each distribution adding up to one!\n",
    "\n",
    "*Remark:* The factor of 2 accounts for the above observation. For example, given two mutually exclusive distributions with only two categories, the TVD would be:\n",
    "\n",
    "$$|1-0| + |0-1| = 1 + 1 = 2$$\n",
    "\n",
    "However, as the second term is completely determined by the first, the more natural quantity is half of the total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov–Smirnov Test Statistic\n",
    "\n",
    "While the difference in means measures the similarity of two quantitative distributions, it declares any two distributions the same as long as their means are the same. Having a measure of similarity that compares the overall shape of the distribution provides a more refined notion of similarity.\n",
    "\n",
    "The Kolmogorov–Smirnov test statistic (KS test statistic) measures the similarity between two quantitative distributions, analogous to the TVD. It asks how different can two distributions be among all ranges of values.\n",
    "\n",
    "The KS-statistic of two distributions is defined as the maximum distance between their empirical CDFs.\n",
    "\n",
    "In the case of the DataFrame `table`, with values `A` and `B` in the `Group` column, the KS-statistic is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two sample\n",
    "sampleA = table.loc[table['Group'] == 'A', 'Value']\n",
    "sampleB = table.loc[table['Group'] == 'B', 'Value']\n",
    "\n",
    "# two  CDFs\n",
    "cdfA = sampleA.value_counts(normalize=True).sort_index().cumsum()\n",
    "cdfB = sampleB.value_counts(normalize=True).sort_index().cumsum()\n",
    "\n",
    "# calculate the maximum distance between CDFs\n",
    "ks = (\n",
    "    pd.concat([cdfA, cdfB], axis=1)\n",
    "    .sort_index()\n",
    "    .fillna(method='ffill')\n",
    "    .fillna(0)\n",
    "    .diff(axis=1)\n",
    "    .iloc[:, -1]\n",
    "    .abs()\n",
    "    .max()\n",
    ")\n",
    "\n",
    "ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark:* This method is both space-inefficient and logically opaque, but is a nice application of table manipulation. Walk through the methods step-by-step. How is this equivalent to using the function definition of the empirical CDFs in the last chapter? (Convince yourself!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark:* The `scipy.stats` library provides the function `ks_2samp` that runs a KS-test whose results are similar to the permutation test described in this section. The function returns:\n",
    "* the value of the KS-statistic (as computed above), \n",
    "* the p-value resulting from the hypothesis test with the null-hypothesis: \"the two samples come from the same distribution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}