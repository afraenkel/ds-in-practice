
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>When is a Model a Good Model? (Prediction) &#8212; Data Science in Practice</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Cross Validation" href="cross-validation.html" />
    <link rel="prev" title="When is a Model a Good Model? (Inference)" href="fitting-inference.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/toolbox.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Data Science in Practice</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Data Science in Practice
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Understanding Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../01/introduction.html">
   Introduction to Data Science
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../01/what-is-data-science.html">
     What is Data Science?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/scientific-method.html">
     The Scientific Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/data-science-example.html">
     A Data Science Example
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../02/introduction.html">
   The Basics of Tabular Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../02/tabular-data.html">
     Tabular Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/tabular-data.html#tables-in-python-using-pandas">
     Tables in python, using Pandas.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/methods.html">
     Table Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data-types.html">
     Data Types and Performance
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../03/introduction.html">
   Querying and Describing Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../03/selecting-data.html">
     Selecting Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/kinds-of-data.html">
     Kinds of Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/categorical-distributions.html">
     Categorical Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/quantitative-distributions.html">
     Quantitative Distributions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../04/introduction.html">
   Understanding Assumptions and Data Cleaning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../04/modifying-dataframes.html">
     Modifying DataFrames
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/cleaning.html">
     Cleaning Messy Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/eda.html">
     Exploratory Data Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/asking-questions.html">
     Hypothesis Testing
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../05/introduction.html">
   Aggregation and Extension of Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../05/grouping.html">
     Data Granularity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/understanding-aggregations.html">
     Understanding Aggregations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/appending-data.html">
     Combining Data (Observations)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/joining-data.html">
     Combining Data: Attributes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/joining-data.html#combining-different-measurements-over-the-same-individuals">
     Combining different measurements over the same individuals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/permutation-tests.html">
     Permutation Tests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/eda-2.html">
     Exploratory Data Analysis II
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../06/introduction.html">
   Missing Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../06/defining-missing.html">
     Definitions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/identifying-missing.html">
     Identifying Missing Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/handling-missing-data.html">
     Handling Missing Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/single-valued-imputation.html">
     Single-Valued Imputation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/probabilistic-imputation.html">
     Probabilistic Imputation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Collecting Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../07/introduction.html">
   Data Collection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../07/existing-data.html">
     Using Existing Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/requests.html">
     HTTP Requests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/html.html">
     Parsing HTML
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../08/introduction.html">
   Information Extaction
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../08/patterns.html">
     Text Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/nlp.html">
     Natural Language Processing
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../09/introduction.html">
   Introduction to Features
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../09/features.html">
     Feature Engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/data-pipelines.html">
     Data Pipelines
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Modeling With Data
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../10/introduction.html">
   Modeling Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../10/intro-modeling.html">
     Introduction to Statistical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/model-building.html">
     Building Modeling Pipelines
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="introduction.html">
   Bias and Variance
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="fitting-inference.html">
     Model Quality (Inference)
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Model Quality (Prediction)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cross-validation.html">
     Cross Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="parameter-search.html">
     Parameter Search
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12/introduction.html">
   Evaluating Models; Fairness
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12/eval.html">
     Evaluation metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/parity.html">
     Parity Measures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/fairness.html">
     Fairness in Machine Learning
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/11/fitting-prediction.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/afraenkel/ds-in-practice/master?urlpath=tree/book/content/11/fitting-prediction.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias">
   Bias
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance">
   Variance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-trade-off">
   Bias-Variance Trade-off
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-bias-variance-possibilities">
     The bias-variance possibilities
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-test-split">
   Train-Test Split
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-test-split-in-scikit-learn">
     Train-test split in Scikit-Learn
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="when-is-a-model-a-good-model-prediction">
<h1>When is a Model a Good Model? (Prediction)<a class="headerlink" href="#when-is-a-model-a-good-model-prediction" title="Permalink to this headline">¶</a></h1>
<hr class="docutils" />
<p>Unlike a model fit for inference, a predictive model need not reveal truths about the data generating process; it need only predict the values being generated.</p>
<p>The quality of a predictor is explained using the concepts of <em>bias</em> and <em>variance</em>. While, these terms relate to their statistical counterparts, they are used in a slightly different manner.</p>
<div class="section" id="bias">
<h2>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h2>
<p>A model with high bias gives systematically incorrect predictions and cannot capture the complexity of the data generating process.</p>
<p><strong>Definition:</strong> The bias of a predictive model measures the average deviation of the predictions from the true values. The deviation is typically measured using the loss function over which the fit model was minimized. A model with high bias is said to <em>under-fit</em> the data.</p>
<p>Bias in a predictor is identified by the inability of the model to predict the data on which it was fit – i.e. the loss function over which the fit model was minimized is large.</p>
<p>High bias, or under-fitting, is handled by adding complexity to the modeling pipeline. This may be done by:</p>
<ol class="simple">
<li><p>Adding features to the model that captures the un-modeled complexity,</p></li>
<li><p>Using a different model type that is capable of capturing the un-modeled complexity.</p></li>
</ol>
<p><strong>Example:</strong> Consider the problem of predicting a restaurant tip amount from the total bill. A simple constant model (“every party tips exactly $3”) is too simple to capture the trend between the variables. It suffers from high bias:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tips</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">tips</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Constant Model: Every Tip is $3.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;predictions&#39;</span><span class="p">,</span> <span class="s1">&#39;tip amounts&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fitting-prediction_3_0.png" src="../../_images/fitting-prediction_3_0.png" />
</div>
</div>
<p>The RMSE of such a model is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">tips</span><span class="o">.</span><span class="n">tip</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># Bias</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.3808010267268185
</pre></div>
</div>
</div>
</div>
<p>Among all constant models, the one with the least bias is the one that always predicts the mean (prove this!) – in this case ~$3. However, consider a more complex model type can decrease the bias: a constant percentage model more capably captures the relationship between the variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tips</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">tips</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Percentage Model: Tip 15</span><span class="si">% o</span><span class="s1">f Total Bill&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span> <span class="o">*</span> <span class="mf">0.15</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;predictions&#39;</span><span class="p">,</span> <span class="s1">&#39;tip amounts&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fitting-prediction_7_0.png" src="../../_images/fitting-prediction_7_0.png" />
</div>
</div>
<p>The RMSE of the 15% model is lower than the constant model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="mf">0.15</span> <span class="o">*</span> <span class="n">tips</span><span class="o">.</span><span class="n">total_bill</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">preds</span> <span class="o">-</span> <span class="n">tips</span><span class="o">.</span><span class="n">tip</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">rmse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0938913032845812
</pre></div>
</div>
</div>
</div>
<p><em>Remark:</em> As seen in the previous section, the linear regression model fit to total bill and tip amount will be a better estimator than the 15% model!</p>
</div>
<div class="section" id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h2>
<p>A model with high variance doesn’t generalize well to new datasets. This is the case when a model has learned the observed data instead of the learning the data generating process.</p>
<p><strong>Definition:</strong> For a specified model, the change in the parameters of the fit model as a function of new sample data, is the variance of the model. If the parameters of a model change significantly on out-of-sample data, then the model has high variance and is said to <em>over-fit</em> the data.</p>
<p>High variance in a predictor is identified by evaluating the bias of the model on new unseen data – if the observed bias of the model significantly increases on new data, then the model is over-fit to the original sample.</p>
<p>High variance, or over-fitting, is handled by decreasing the complexity of the modeling pipeline.</p>
<p><strong>Example:</strong> Fitting a polynomial model to the tips dataset, yields a predictor that likely “connects the dots” – learning the training data, as opposed to a real relationship between the variables:</p>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">,</span> <span class="n">a4</span><span class="p">,</span> <span class="n">a5</span><span class="p">,</span> <span class="n">a6</span><span class="p">,</span> <span class="n">a7</span><span class="p">,</span> <span class="n">a8</span><span class="p">,</span> <span class="n">a9</span><span class="p">):</span>

    <span class="n">locs</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">locs</span> <span class="k">if</span> <span class="s1">&#39;a&#39;</span> <span class="o">==</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]]))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">locs</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">params</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
<span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">tips</span><span class="o">.</span><span class="n">total_bill</span><span class="p">,</span> <span class="n">tips</span><span class="o">.</span><span class="n">tip</span><span class="p">)</span>

<span class="n">tips</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">tips</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Degree-9 Polynomial Predictor&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tips</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="n">func</span><span class="p">(</span><span class="n">tips</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="o">*</span><span class="n">params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fitting-prediction_13_0.png" src="../../_images/fitting-prediction_13_0.png" />
</div>
</div>
<p>Bootstrapping the data to simulate new draws from the same population illustrates the variation in the resulting fit polynomial models on the datasets. Notice, where data are sparse or outliers are present, the resulting predictors vary more:</p>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tips</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">tips</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Degree-9 Polynomial Predictor (50 Bootstrappred Models)&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tips</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">tip</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="n">func</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="o">*</span><span class="n">params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fitting-prediction_15_0.png" src="../../_images/fitting-prediction_15_0.png" />
</div>
</div>
<p>Reducing the model complexity yields model parameters that vary less:</p>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span><span class="p">):</span>

    <span class="n">locs</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">locs</span> <span class="k">if</span> <span class="s1">&#39;a&#39;</span> <span class="o">==</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]]))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">locs</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">params</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="n">tips</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">tips</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Degree-3 Polynomial Predictor (50 Bootstrappred Models)&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">tips</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">tip</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="n">func</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">total_bill</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(),</span> <span class="o">*</span><span class="n">params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fitting-prediction_17_0.png" src="../../_images/fitting-prediction_17_0.png" />
</div>
</div>
</div>
<div class="section" id="bias-variance-trade-off">
<h2>Bias-Variance Trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permalink to this headline">¶</a></h2>
<p>In prediction problems, the concepts of bias and variance are in tension. This tension is known as the <em>bias-variance trade-off</em>, and can be quantified with the statistical notions of bias and variance.</p>
<p>In regression problems, the loss-function often being minimized is the (root)-mean-squared-error (MSE). This quantity can be decomposed into bias and variance, allowing one to understand the minimization of the loss-function in terms of these fundamental measures of model quality.</p>
<p>Suppose a data generating process takes on form of <span class="math notranslate nohighlight">\(y = f(x) + \epsilon\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> are independent variables, <span class="math notranslate nohighlight">\(f\)</span> is the idealized process generating the data, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is a noise term (with mean <span class="math notranslate nohighlight">\(0\)</span> and standard-deviation <span class="math notranslate nohighlight">\(\sigma^2\)</span>). A fit model <span class="math notranslate nohighlight">\(\hat f\)</span> minimizes the square of the expected error (MSE):</p>
<div class="math notranslate nohighlight">
\[\rm{MSE} = E[(y-\hat f(x))^2] = \rm{Bias}[\hat f(x)]^2 + \rm{Var}[\hat f(x)] + \sigma^2\]</div>
<p>Where the bias is given by <span class="math notranslate nohighlight">\(\rm{Bias}[\hat f(x)] = E[\hat f(x)] - E[y]\)</span>, and the variance is given by <span class="math notranslate nohighlight">\(\rm{Var}[\hat f] = E[\hat f(x)^2] - E[\hat f(x)]^2\)</span>. For a derivation of this decomposition see <a class="reference external" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">here</a>.</p>
<p>Thus, the error being minimized decomposes into three non-negative terms:</p>
<ul class="simple">
<li><p>The square of the bias, driven by the errors in the ability to make predictions on the observed data.</p></li>
<li><p>The variance captures the tendency for the predictions to deviate from their average. This quantity may be large, even if the (signed) errors are close on average.</p></li>
<li><p>The irreducible error <span class="math notranslate nohighlight">\(\sigma^2\)</span> represents the variation in the observed data from noise in the system. Such noise is unknowable; a perfect model will still have error equal to <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
</ul>
<div class="section" id="the-bias-variance-possibilities">
<h3>The bias-variance possibilities<a class="headerlink" href="#the-bias-variance-possibilities" title="Permalink to this headline">¶</a></h3>
<p>The bias-variance decomposition explain <em>how</em> model error presents itself and suggests ways to improve model performance. A model may exhibit varying combinations of bias and variance, under which the model may be explained:</p>
<ul class="simple">
<li><p><em>Low-Bias / Low-Variance</em>: A good model that makes accurate prediction and captures the data generating process well.</p></li>
<li><p><em>High-Bias / Low-Variance:</em> A (under-fit) model that incorrectly predicts the events in a predictable way. This situation might occur from using a model pipeline that’s too simple to describe the complex relationships in the data.</p></li>
<li><p><em>Low-Bias / High-Variance:</em> A (over-fit) model that memorizes the observed data without capturing the broader patterns of the data generating process.</p></li>
<li><p><em>High-Bias / High-Variance:</em> A model that is both over and under-fit to data. This might happen by choosing a very inappropriate model type, that over-fits on a subset of the observed data.</p></li>
</ul>
<p><em>Remark:</em> The observations above assume that the model is fit on a <em>representative sample</em>. Fitting a model on <em>bad data</em> might appear to create a very good low-bias, low-variance model, that is in fact a high-bias, low-variance model. This situation is especially dangerous, as it leads one to be very confident in a very bad model!</p>
</div>
</div>
<div class="section" id="train-test-split">
<h2>Train-Test Split<a class="headerlink" href="#train-test-split" title="Permalink to this headline">¶</a></h2>
<p>In practice, how does one assess if a given model suffers from high bias or high variance?</p>
<ul class="simple">
<li><p>Bias is easily identified by looking at the error when fitting the model on observed data.</p></li>
<li><p>Variance requires comparing the sensitivity of a model across datasets.</p></li>
</ul>
<p>The assessment of the variance of a fit model can be done by reserving part of the observed data to act as this “other dataset” on which one can evaluate the fit model. This procedure is referred to as the <em>train-test split</em> of the data.</p>
<p><strong>Definition:</strong> Given observed data on which a model is fit, a <em>train-test split</em> of the observed data is a random split of the dataset into two datasets (called ‘training data’ and ‘test data’) on which:</p>
<ul class="simple">
<li><p>the training data is used to fit the model, and</p></li>
<li><p>the test data is used to evaluate the model.</p></li>
</ul>
<p>A decrease in quality of a model between the training set (on which it was fit) and the test set likely means the model has high variance and <em>over-fit</em> the training data.</p>
<div class="section" id="train-test-split-in-scikit-learn">
<h3>Train-test split in Scikit-Learn<a class="headerlink" href="#train-test-split-in-scikit-learn" title="Permalink to this headline">¶</a></h3>
<p>In practice, the train-test split requires a number of choices. For example:</p>
<ul class="simple">
<li><p>How large should the test set be? The size of Train:Test is 3:1?</p></li>
<li><p>How should the data be split? Uniformly? or in a more subtle way?</p></li>
</ul>
<p>In general the larger the test set, the more reliable the estimate of performance on unseen data. However, too small of a training set might lead to higher bias in the model. Most commonly, the train-test split is done uniformly across observations, reserving a test set that is approximately 10-33% of the total data.</p>
<p>Scikit-learn has a function to do a train-test split an observed dataset <span class="math notranslate nohighlight">\(X, y\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When building a model using a train-test split:</p>
<ul class="simple">
<li><p>The training set is used to fit a model with the lowest possible bias,</p></li>
<li><p>The test set gives an estimate of the fit models performance on unseen data.</p></li>
</ul>
<p>It’s important that the test set is <em>totally independent</em> from the training set.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/11"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="fitting-inference.html" title="previous page">When is a Model a Good Model? (Inference)</a>
    <a class='right-next' id="next-link" href="cross-validation.html" title="next page">Cross Validation</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Aaron Fraenkel<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>